{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2f9e9",
   "metadata": {},
   "source": [
    "1. 參數設定 (Configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/task1_dataset_kotae.csv\"\n",
    "MODEL_SAVE_DIR = \"../models\"\n",
    "MODEL_SAVE_PATH = f\"{MODEL_SAVE_DIR}/seq2seq_multivariate_timeperiod.pth\"\n",
    "SCALER_SAVE_PATH = f\"{MODEL_SAVE_DIR}/scaler_multivariate_timeperiod.pkl\"\n",
    "LOG_SAVE_PATH = f\"{MODEL_SAVE_DIR}/eval_log_multivariate_timeperiod.txt\"\n",
    "\n",
    "# 模型超參數（與 model1, multivariate 相同）\n",
    "INPUT_SEQ_LEN = 144   # 輸入過去 72 小時\n",
    "OUTPUT_SEQ_LEN = 48   # 預測未來 24 小時\n",
    "BATCH_SIZE = 512\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 4\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20  # Early Stopping 耐心值\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# 特徵維度設定\n",
    "# [人數, is_weekend, time_period_0, time_period_1, time_period_2, time_period_3]\n",
    "# 時段使用 One-Hot Encoding (4維)，所以總共 1 + 1 + 4 = 6 維\n",
    "INPUT_SIZE = 6        \n",
    "OUTPUT_SIZE = 1       # [人數]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b77f84",
   "metadata": {},
   "source": [
    "2. 資料處理與標籤生成 (Data Processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_period(t):\n",
    "    \"\"\"\n",
    "    根據時間點 t (0-47) 分類為不同時段\n",
    "    每個 t 代表 30 分鐘，所以：\n",
    "    - t=0 代表 00:00, t=1 代表 00:30, ...\n",
    "    - t=16 代表 08:00, t=24 代表 12:00, ...\n",
    "    \n",
    "    時段分類：\n",
    "    - 0: 深夜 (00:00 - 06:00) -> t: 0-11\n",
    "    - 1: 早上 (06:00 - 12:00) -> t: 12-23\n",
    "    - 2: 下午 (12:00 - 18:00) -> t: 24-35\n",
    "    - 3: 晚上 (18:00 - 24:00) -> t: 36-47\n",
    "    \"\"\"\n",
    "    if t < 12:\n",
    "        return 0  # 深夜\n",
    "    elif t < 24:\n",
    "        return 1  # 早上\n",
    "    elif t < 36:\n",
    "        return 2  # 下午\n",
    "    else:\n",
    "        return 3  # 晚上\n",
    "\n",
    "def load_and_preprocess_data(path):\n",
    "    print(\"Loading raw data...\")\n",
    "    raw_df = pd.read_csv(path)\n",
    "    \n",
    "    # 先將原始資料聚合算出人數\n",
    "    print(\"Aggregating data to calculate 'number of people'...\")\n",
    "    df = raw_df.groupby(['d', 't', 'x', 'y']).size().reset_index(name='number of people')\n",
    "    \n",
    "    print(f\"Aggregated data shape: {df.shape}\")\n",
    "    \n",
    "    # A. 自動生成 Weekend 標籤 (K-Means)\n",
    "    print(\"Generating 'is_weekend' labels using K-Means...\")\n",
    "    \n",
    "    # 選擇總人數最多的網格作為基準\n",
    "    top_grid_idx = df.groupby(['x', 'y'])['number of people'].sum().idxmax()\n",
    "    print(f\"Base grid for clustering: {top_grid_idx}\")\n",
    "    \n",
    "    base_df = df[(df['x'] == top_grid_idx[0]) & (df['y'] == top_grid_idx[1])].copy()\n",
    "    \n",
    "    # 轉成矩陣: Index=天數, Columns=時間點\n",
    "    pivot_matrix = base_df.pivot(index='d', columns='t', values='number of people').fillna(0)\n",
    "    \n",
    "    # K-Means 分群\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(pivot_matrix)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # 判斷哪一群是週末\n",
    "    c0_idx = np.where(labels == 0)[0]\n",
    "    c1_idx = np.where(labels == 1)[0]\n",
    "    \n",
    "    # 比較 t=16 (早上8點) 的平均人流\n",
    "    if len(c0_idx) > 0 and len(c1_idx) > 0:\n",
    "        avg_flow_0 = pivot_matrix.iloc[c0_idx, 16].mean()\n",
    "        avg_flow_1 = pivot_matrix.iloc[c1_idx, 16].mean()\n",
    "        weekend_label_cluster = 0 if avg_flow_0 < avg_flow_1 else 1\n",
    "    else:\n",
    "        # 極端情況處理\n",
    "        weekend_label_cluster = 1 if len(c0_idx) < len(c1_idx) else 0\n",
    "\n",
    "    # 建立 mapping\n",
    "    day_is_weekend = [1 if l == weekend_label_cluster else 0 for l in labels]\n",
    "    label_map = pd.DataFrame({'d': pivot_matrix.index, 'is_weekend': day_is_weekend})\n",
    "    \n",
    "    print(f\"Weekday count: {len(label_map[label_map['is_weekend']==0])}\")\n",
    "    print(f\"Weekend count: {len(label_map[label_map['is_weekend']==1])}\")\n",
    "    \n",
    "    # B. 生成時段標籤 (One-Hot 編碼)\n",
    "    print(\"Generating 'time_period' labels with One-Hot encoding...\")\n",
    "    df['time_period'] = df['t'].apply(get_time_period)\n",
    "    \n",
    "    # One-Hot Encoding：建立 4 個欄位\n",
    "    df['time_period_0'] = (df['time_period'] == 0).astype(float)  # 深夜\n",
    "    df['time_period_1'] = (df['time_period'] == 1).astype(float)  # 早上\n",
    "    df['time_period_2'] = (df['time_period'] == 2).astype(float)  # 下午\n",
    "    df['time_period_3'] = (df['time_period'] == 3).astype(float)  # 晚上\n",
    "    \n",
    "    period_counts = df['time_period'].value_counts().sort_index()\n",
    "    print(f\"時段分布: 深夜(0)={period_counts.get(0, 0)}, 早上(1)={period_counts.get(1, 0)}, 下午(2)={period_counts.get(2, 0)}, 晚上(3)={period_counts.get(3, 0)}\")\n",
    "    print(\"使用 One-Hot 編碼表示時段特徵 (4維)\")\n",
    "    \n",
    "    # C. 篩選前三大熱點並合併標籤\n",
    "    print(\"Selecting Top 3 locations...\")\n",
    "    top_3 = df.groupby(['x', 'y'])['number of people'].sum().nlargest(3).reset_index()[['x', 'y']]\n",
    "    \n",
    "    # 只保留這三個地點的資料\n",
    "    result_df = pd.merge(df, top_3, on=['x', 'y'], how='inner')\n",
    "    \n",
    "    # 合併 K-Means 產生的標籤\n",
    "    result_df = pd.merge(result_df, label_map, on='d', how='left')\n",
    "    \n",
    "    # D. 標準化 (Normalization)\n",
    "    scaler = MinMaxScaler()\n",
    "    result_df['number_scaled'] = scaler.fit_transform(result_df[['number of people']])\n",
    "    \n",
    "    return result_df, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f140d",
   "metadata": {},
   "source": [
    "3. 自定義 Dataset (Multivariate with One-Hot Time Period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6395dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, group_by_cols, target_col, input_seq_len, output_seq_len):\n",
    "        \"\"\"\n",
    "        使用 One-Hot 編碼的時段特徵\n",
    "        輸入特徵: [人數, is_weekend, time_period_0, time_period_1, time_period_2, time_period_3]\n",
    "        \"\"\"\n",
    "        self.sequences = []\n",
    "        \n",
    "        grouped = df.groupby(group_by_cols)\n",
    "        \n",
    "        for _, group_df in grouped:\n",
    "            # 按 ['d', 't'] 排序\n",
    "            group_df = group_df.sort_values(['d', 't'])\n",
    "            \n",
    "            target_vals = group_df[target_col].values\n",
    "            is_weekend_vals = group_df['is_weekend'].values\n",
    "            tp0_vals = group_df['time_period_0'].values\n",
    "            tp1_vals = group_df['time_period_1'].values\n",
    "            tp2_vals = group_df['time_period_2'].values\n",
    "            tp3_vals = group_df['time_period_3'].values\n",
    "            \n",
    "            total_len = len(target_vals)\n",
    "            \n",
    "            # 滑動視窗生成序列\n",
    "            for i in range(total_len - input_seq_len - output_seq_len + 1):\n",
    "                in_target = target_vals[i : i + input_seq_len]\n",
    "                in_weekend = is_weekend_vals[i : i + input_seq_len]\n",
    "                in_tp0 = tp0_vals[i : i + input_seq_len]\n",
    "                in_tp1 = tp1_vals[i : i + input_seq_len]\n",
    "                in_tp2 = tp2_vals[i : i + input_seq_len]\n",
    "                in_tp3 = tp3_vals[i : i + input_seq_len]\n",
    "                \n",
    "                # Stack: (Seq_Len, 6) - 人數 + is_weekend + 4個 One-Hot 時段\n",
    "                input_seq = np.stack((in_target, in_weekend, in_tp0, in_tp1, in_tp2, in_tp3), axis=1)\n",
    "                \n",
    "                # Output: (Output_Len)\n",
    "                output_seq = target_vals[i + input_seq_len : i + input_seq_len + output_seq_len]\n",
    "                \n",
    "                self.sequences.append((input_seq, output_seq))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, output_seq = self.sequences[idx]\n",
    "        return torch.FloatTensor(input_seq), torch.FloatTensor(output_seq).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3a1a0",
   "metadata": {},
   "source": [
    " 4. 模型架構 (Seq2Seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75004acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, target_len, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_len = target_len\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source):\n",
    "        batch_size = source.shape[0]\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # Decoder 初始輸入設為 0\n",
    "        decoder_input = torch.zeros(batch_size, 1, 1).to(self.device)\n",
    "        \n",
    "        outputs = []\n",
    "        for _ in range(self.target_len):\n",
    "            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs.append(prediction)\n",
    "            decoder_input = prediction \n",
    "            \n",
    "        outputs = torch.cat(outputs, dim=1) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f1735",
   "metadata": {},
   "source": [
    "5. 主執行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: 準備資料\n",
    "    df, scaler = load_and_preprocess_data(DATA_PATH)\n",
    "    \n",
    "    # 固定切分：40 天訓練，10 天驗證，25 天測試\n",
    "    TRAIN_DAYS = 40\n",
    "    VAL_DAYS = 10\n",
    "    TEST_DAYS = 25\n",
    "    TOTAL_DAYS = 75\n",
    "    \n",
    "    # 重新建立 Dataset：分別為訓練集、驗證集和測試集\n",
    "    train_df = df[df['d'] < TRAIN_DAYS]\n",
    "    val_df = df[(df['d'] >= TRAIN_DAYS) & (df['d'] < TRAIN_DAYS + VAL_DAYS)]\n",
    "    test_df = df[df['d'] >= TRAIN_DAYS + VAL_DAYS]\n",
    "    \n",
    "    print(f\"訓練集天數: 0 ~ {TRAIN_DAYS-1} (共 {TRAIN_DAYS} 天)\")\n",
    "    print(f\"驗證集天數: {TRAIN_DAYS} ~ {TRAIN_DAYS+VAL_DAYS-1} (共 {VAL_DAYS} 天)\")\n",
    "    print(f\"測試集天數: {TRAIN_DAYS+VAL_DAYS} ~ {TOTAL_DAYS-1} (共 {TEST_DAYS} 天)\")\n",
    "    \n",
    "    print(\"Creating dataset with One-Hot time period features...\")\n",
    "    train_dataset = GridTimeSeriesDataset(\n",
    "        train_df, \n",
    "        group_by_cols=['x', 'y'],\n",
    "        target_col='number_scaled',\n",
    "        input_seq_len=INPUT_SEQ_LEN,\n",
    "        output_seq_len=OUTPUT_SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    val_dataset = GridTimeSeriesDataset(\n",
    "        val_df, \n",
    "        group_by_cols=['x', 'y'],\n",
    "        target_col='number_scaled',\n",
    "        input_seq_len=INPUT_SEQ_LEN,\n",
    "        output_seq_len=OUTPUT_SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    test_dataset = GridTimeSeriesDataset(\n",
    "        test_df, \n",
    "        group_by_cols=['x', 'y'],\n",
    "        target_col='number_scaled',\n",
    "        input_seq_len=INPUT_SEQ_LEN,\n",
    "        output_seq_len=OUTPUT_SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"Error: Train dataset is empty. Please check input sequence length and data continuity.\")\n",
    "        exit()\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Step 2: 建立模型\n",
    "    encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)\n",
    "    decoder = Decoder(OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)\n",
    "    model = Seq2Seq(encoder, decoder, OUTPUT_SEQ_LEN, DEVICE).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Step 3: 訓練迴圈 (含 Early Stopping)\n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # 驗證階段：使用驗證集計算 validation loss\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                output = model(x)\n",
    "                val_loss = criterion(output, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early Stopping 檢查\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"Best model updated (Val Loss: {best_val_loss:.6f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
    "            \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "    \n",
    "    # 載入最佳模型並儲存\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'hyperparameters': {\n",
    "            'input_size': INPUT_SIZE,\n",
    "            'hidden_size': HIDDEN_SIZE,\n",
    "            'num_layers': NUM_LAYERS,\n",
    "            'input_seq_len': INPUT_SEQ_LEN,\n",
    "            'output_seq_len': OUTPUT_SEQ_LEN\n",
    "        }\n",
    "    }, MODEL_SAVE_PATH)\n",
    "            \n",
    "    print(f\"\\nTraining complete. Best model saved to {MODEL_SAVE_PATH}\")\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "    \n",
    "    # Step 4: 評估模型（使用測試集\n",
    "    print(\"\\n--- 模型評估 ---\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            output = model(x)\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds, axis=0).reshape(-1, 1)\n",
    "    targets = np.concatenate(all_targets, axis=0).reshape(-1, 1)\n",
    "    \n",
    "    preds_original = scaler.inverse_transform(preds).flatten()\n",
    "    targets_original = scaler.inverse_transform(targets).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(targets_original, preds_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets_original, preds_original)\n",
    "    \n",
    "    print(f\"模型評估結果 (測試集):\")\n",
    "    print(f\"   MSE: {mse:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    \n",
    "    # 儲存評估結果到 log 檔\n",
    "    with open(LOG_SAVE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(\"模型: Multivariate + Time Period Seq2Seq (One-Hot)\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"輸入特徵: [人數, is_weekend, 時段 One-Hot (4維)]\\n\")\n",
    "        f.write(f\"輸入特徵數: {INPUT_SIZE}\\n\")\n",
    "        f.write(f\"訓練集: 前 50 天 (樣本數: {len(train_dataset)})\\n\")\n",
    "        f.write(f\"測試集: 後 25 天 (樣本數: {len(test_dataset)})\\n\")\n",
    "        f.write(\"\\n--- 測試集評估結果 ---\\n\")\n",
    "        f.write(f\"MSE:  {mse:.4f}\\n\")\n",
    "        f.write(f\"RMSE: {rmse:.4f}\\n\")\n",
    "        f.write(f\"MAE:  {mae:.4f}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "    print(f\"評估結果已儲存至 {LOG_SAVE_PATH}\")\n",
    "    \n",
    "    # Step 5: 畫圖（使用測試集）\n",
    "    model.eval()\n",
    "    \n",
    "    # 在測試集中搜尋人流波動明顯的樣本\n",
    "    total_len = len(test_dataset)\n",
    "    print(f\"測試集總數: {total_len}\")\n",
    "    \n",
    "    # 搜尋熱門時段 (High-traffic sample)\n",
    "    target_sample_idx = 0\n",
    "    found = False\n",
    "    \n",
    "    print(\"正在搜尋人流波動明顯的樣本 (Max > 80)...\")\n",
    "    \n",
    "    for i in range(total_len):\n",
    "        _, target_tensor = test_dataset[i]\n",
    "        # 只取人數部分（第一個特徵）進行反標準化\n",
    "        temp_val = scaler.inverse_transform(target_tensor.numpy().reshape(-1, 1))\n",
    "        \n",
    "        if temp_val.max() > 80:\n",
    "            target_sample_idx = i\n",
    "            print(f\"找到目標樣本 Index: {i} (最大人流: {temp_val.max():.2f})\")\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        print(\"未找到 > 80 的樣本，將使用測試集的第一筆資料繪圖。\")\n",
    "    \n",
    "    # 取得樣本進行預測\n",
    "    sample_input, sample_target = test_dataset[target_sample_idx]\n",
    "    sample_input = sample_input.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(sample_input).cpu().numpy().reshape(-1, 1)\n",
    "        target = sample_target.numpy().reshape(-1, 1)\n",
    "        \n",
    "    pred_orig = scaler.inverse_transform(prediction)\n",
    "    target_orig = scaler.inverse_transform(target)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(target_orig, label='Actual (Ground Truth)', linewidth=2)\n",
    "    plt.plot(pred_orig, label='Predicted (Multivariate + Time Period One-Hot)', linestyle='--', color='orange', linewidth=2)\n",
    "    plt.title(f\"Multivariate + Time Period (One-Hot) Seq2Seq Prediction (Sample Index: {target_sample_idx})\")\n",
    "    plt.xlabel(\"Time Steps (Next 24 Hours)\")\n",
    "    plt.ylabel(\"Number of People\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f\"{MODEL_SAVE_DIR}/prediction_result_multivariate_timeperiod.png\")\n",
    "    plt.show()\n",
    "    print(f\"Result plot saved to {MODEL_SAVE_DIR}/prediction_result_multivariate_timeperiod.png\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
